{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU6IhX0eTPt2"
      },
      "source": [
        "# Mapping of human-wildlife conflict in India\n",
        "## This script is for visual representation of data related to human-wildlife conflict in India. The conflict data has been sourced from two sources: journal articles sourced from PubMed Central (https://www.ncbi.nlm.nih.gov/pmc/) and GoogleNews portal. The process has been described below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script take resources from and save outputs to following Google Drive folder:\n",
        "https://drive.google.com/drive/u/0/folders/1VrRL4Nc3AYb8neCeYJ_99jQ3nrC_p8vw\n",
        "\n",
        "To run the script on local machine, download the Google Drive folder (https://drive.google.com/drive/u/0/folders/1VrRL4Nc3AYb8neCeYJ_99jQ3nrC_p8vw) and change the path of folder path and outpur directory in the script to the location of downloaded folder."
      ],
      "metadata": {
        "id": "jYJzBp7FpOto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nejaQ9W-VyG6"
      },
      "outputs": [],
      "source": [
        "# creating a virtual environment for the project.\n",
        "\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "!source myenv/bin/activate\n",
        "\n",
        "# Installing the required libraries\n",
        "\n",
        "!pip install pandas numpy geopandas requests datetime Biopython newspaper3k serpapi google-search-results spacy geopy folium\n",
        "\n",
        "# Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "from datetime import timedelta\n",
        "import csv\n",
        "import json\n",
        "from Bio import Entrez\n",
        "import newspaper\n",
        "from serpapi import GoogleSearch\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "from tqdm import tqdm\n",
        "from geopy.geocoders import Nominatim\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "# Setting up Google Drive folder to store the data\n",
        "# code4Nature = https://drive.google.com/drive/u/0/folders/1VrRL4Nc3AYb8neCeYJ_99jQ3nrC_p8vw\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = \"/content/drive/MyDrive/code4Nature\"  # change the path if you are running on your local machine\n",
        "output_dir = \"/content/drive/My Drive/code4Nature\"  # change the path if you are running on your local machine\n",
        "\n",
        "# Define Geocoder\n",
        "geolocator = Nominatim(user_agent=\"my-geocoding-app\")\n",
        "\n",
        "# Downloading spacy Large English model and defining a nlp object\n",
        "# source:  https://spacy.io/ , https://spacy.io/usage/models\n",
        "\n",
        "! python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load(\"en_core_web_lg\") # inbuilt NER model for location\n",
        "\n",
        "nlp_hwc = spacy.load(output_dir) # custom NER model for human-wildlife conflict\n",
        "\n",
        "\n",
        "### Content search starts here\n",
        "# Searching PubMed database (https://www.ncbi.nlm.nih.gov/pmc/) to get the scholarly articles\n",
        "# with keyword 'human-wildlife conflict india' in 'Title/Abstract'.\n",
        "# Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/\n",
        "\n",
        "def search(query):\n",
        "    Entrez.email = 'shaurabh.anand@apu.edu.in' # Individual account only for testing purpose. Not for production and deployement.\n",
        "    handle = Entrez.esearch(db='pubmed',\n",
        "                            sort='relevance',\n",
        "                            retmax='500',\n",
        "                            retmode='xml',\n",
        "                            term=query,\n",
        "                            field='Title/Abstract')\n",
        "    results = Entrez.read(handle)\n",
        "    return results\n",
        "\n",
        "def fetch_details(id_list):\n",
        "    ids = ','.join(id_list)\n",
        "    Entrez.email = 'shaurabh.anand@apu.edu.in' # Individual account only for testing purpose. Not for production and deployement.\n",
        "    handle = Entrez.efetch(db='pubmed',\n",
        "                           retmode='xml',\n",
        "                           id=ids)\n",
        "    results = Entrez.read(handle)\n",
        "    return results\n",
        "\n",
        "def extract_data(papers):\n",
        "    extracted_data = []\n",
        "    for paper in papers['PubmedArticle']:\n",
        "        title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
        "        abstract = paper['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', '')\n",
        "\n",
        "        # Access the list containing the ELocationID\n",
        "        elocation_list = paper['MedlineCitation']['Article'].get('ELocationID', [])\n",
        "\n",
        "        # Find and return DOI ID (assuming only one DOI)\n",
        "        article_id = 'N/A'\n",
        "        for element in elocation_list:\n",
        "            if element.attributes['EIdType'] == 'doi':\n",
        "                article_id = element\n",
        "                break\n",
        "        base_url = 'https://doi.org/'  # PubMed base URL\n",
        "        link = f\"{base_url}{article_id}\" if article_id != 'N/A' else 'N/A'\n",
        "\n",
        "        extracted_data.append({'Title': title, 'Abstract': abstract, 'Link': link})\n",
        "    return extracted_data\n",
        "\n",
        "def write_to_csv(papers, csv_file):\n",
        "    extracted_data = extract_data(papers)\n",
        "    fieldnames = ['Title', 'Abstract', 'Link']\n",
        "\n",
        "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(extracted_data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Perform search and fetch details\n",
        "    keywords = ['human-wildlife conflict India', 'wildlife crop loss India', 'wildlife crop damage India', 'livestock depredation India', 'wildlife conflict India']\n",
        "    id_list = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        result = search(keyword)\n",
        "        id_list.extend(result['IdList'])\n",
        "\n",
        "    papers = fetch_details(id_list)\n",
        "    write_to_csv(papers, 'hwc_pubmed_articles.csv')\n",
        "\n",
        "pubmed_paper = pd.read_csv(\"hwc_pubmed_articles.csv\")\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Define Geocoder\n",
        "geolocator = Nominatim(user_agent=\"my-geocoding-app\", timeout=10)\n",
        "\n",
        "# Define search and fetch functions for PubMed\n",
        "def nlp_location(abstract, nlp):\n",
        "    doc = nlp(abstract)\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"] # Extract location based on spacy 'Named Entity Recognition (NER) model'\n",
        "    return locations\n",
        "\n",
        "pubmed_paper[\"locations\"] = pubmed_paper[\"Abstract\"].apply(lambda x: nlp_location(x, nlp))\n",
        "\n",
        "# Defining a function to get species data from the abstract\n",
        "def nlp_species(abstract, nlp_hwc):\n",
        "    doc = nlp_hwc(abstract)\n",
        "    species_info = [ent.text for ent in doc.ents if ent.label_ in (\"FAUNA_ENGLISH_NAME\", \"FAUNA_LATIN_NAME\")] # Extract both English and Latin name of animal species based on custom NER model\n",
        "    return species_info\n",
        "\n",
        "pubmed_paper[\"species\"] = pubmed_paper[\"Abstract\"].apply(lambda x: nlp_species(x, nlp_hwc))\n",
        "\n",
        "# Defining a function to get conflict info from the abstract\n",
        "def nlp_conflict(abstract, nlp_hwc):\n",
        "    doc = nlp_hwc(abstract)\n",
        "    conflict_info = [ent.text for ent in doc.ents if ent.label_ == \"CONFLICT_TYPE\"]\n",
        "    return conflict_info\n",
        "\n",
        "pubmed_paper[\"conflict_info\"] = pubmed_paper[\"Abstract\"].apply(lambda x: nlp_conflict(x, nlp_hwc))\n",
        "\n",
        "# Function to geocode a single location\n",
        "def geocode_location(locations):\n",
        "    geocoded = []\n",
        "    for location_text in locations:\n",
        "        try:\n",
        "            location = geolocator.geocode(location_text)\n",
        "            if location:\n",
        "                geocoded.append({\"latitude\": location.latitude, \"longitude\": location.longitude})\n",
        "            else:\n",
        "                geocoded.append({\"latitude\": None, \"longitude\": None})\n",
        "        except (geopy.exc.GeocoderTimedOut, geopy.exc.GeocoderServiceError) as e:\n",
        "            print(f\"Error geocoding location: {location_text} ({type(e)})\")\n",
        "            geocoded.append({\"latitude\": None, \"longitude\": None})\n",
        "    return geocoded\n",
        "\n",
        "# Apply geocoding to the locations column\n",
        "pubmed_paper[\"geocoded\"] = pubmed_paper[\"locations\"].apply(geocode_location)\n",
        "\n",
        "# Saving the final file\n",
        "filename = \"pubmed_df.csv\"\n",
        "pubmed_paper.to_csv(f\"{folder_path}/{filename}\", index=False)\n",
        "\n",
        "# Searching GoogleNews using serpapi. Only for testing\n",
        "# Documentation: https://serpapi.com/google-news-api\n",
        "\n",
        "params = {\n",
        "    \"engine\": \"google_news\",\n",
        "    \"q\": \"human-wildlife-conflict india\",\n",
        "    \"gl\": \"in\",\n",
        "    \"api_key\": \"bedfc950b4dd950b98eb72d44c22e853d16a7d63aaf64c7838f79dde1dbcbc0d\" # Private Key, should not be shared. Supports only 100 searches per month\n",
        "}\n",
        "\n",
        "search = GoogleSearch(params)\n",
        "results = search.get_dict()\n",
        "news_results = results[\"news_results\"]\n",
        "\n",
        "# Convert to dataframe\n",
        "news_df = pd.DataFrame(news_results)\n",
        "\n",
        "def article_text(row):\n",
        "    url = row[\"link\"]\n",
        "    try:\n",
        "        article_obj = newspaper.Article(url)\n",
        "        article_obj.download()\n",
        "        article_obj.parse()\n",
        "        return article_obj.text\n",
        "    except (newspaper.ArticleException, ConnectionError, TimeoutError) as e:\n",
        "        print(f\"Error processing article: {url} ({type(e)})\")\n",
        "        return \"\"\n",
        "\n",
        "news_df[\"article_text\"] = news_df.apply(article_text, axis=1)\n",
        "\n",
        "# Defining a function to extract locations from the article text\n",
        "def nlp_location_article(article_text, nlp):\n",
        "    doc = nlp(article_text)\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"] # Extract location based on spacy 'Named Entity Recognition (NER) model'\n",
        "    return locations\n",
        "\n",
        "news_df[\"locations\"] = news_df[\"article_text\"].apply(lambda x: nlp_location_article(x, nlp))\n",
        "\n",
        "# Defining a function to get species data from the article text\n",
        "def nlp_species_article(article_text, nlp_hwc):\n",
        "    doc = nlp_hwc(article_text)\n",
        "    species_info = [ent.text for ent in doc.ents if ent.label_ in (\"FAUNA_ENGLISH_NAME\", \"FAUNA_LATIN_NAME\")] # Extract both English and Latin name of animal species based on custom NER model\n",
        "    return species_info\n",
        "\n",
        "news_df[\"species\"] = news_df[\"article_text\"].apply(lambda x: nlp_species_article(x, nlp_hwc))\n",
        "\n",
        "# Defining a function to get conflict info from the article text\n",
        "def nlp_conflict_article(article_text, nlp_hwc):\n",
        "    doc = nlp_hwc(article_text)\n",
        "    conflict_info = [ent.text for ent in doc.ents if ent.label_ == \"CONFLICT_TYPE\"]\n",
        "    return conflict_info\n",
        "\n",
        "news_df[\"conflict_info\"] = news_df[\"article_text\"].apply(lambda x: nlp_conflict_article(x, nlp_hwc))\n",
        "\n",
        "# Apply geocoding to the locations column\n",
        "news_df[\"geocoded\"] = news_df[\"locations\"].apply(geocode_location)\n",
        "\n",
        "# Saving the final file\n",
        "filename = \"news_df.csv\"\n",
        "news_df.to_csv(f\"{folder_path}/{filename}\", index=False)\n",
        "\n",
        "### Mapping and visualization block starts here\n",
        "\n",
        "# Define the bounding box for India. Approximate estimate. Not for official publications\n",
        "min_lat, max_lat = 6.0, 37.0\n",
        "min_lon, max_lon = 68.0, 97.0\n",
        "\n",
        "# Function to check if a point is within the bounding box of India\n",
        "def is_within_india(latitude, longitude):\n",
        "    return min_lat <= latitude <= max_lat and min_lon <= longitude <= max_lon\n",
        "\n",
        "# Create the map centered on India\n",
        "m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
        "\n",
        "# Add markers for PubMed data\n",
        "for idx, row in pubmed_paper.iterrows():\n",
        "    for loc in row[\"geocoded\"]:\n",
        "        if loc[\"latitude\"] and loc[\"longitude\"]:\n",
        "            lat, lon = loc[\"latitude\"], loc[\"longitude\"]\n",
        "            if is_within_india(lat, lon):\n",
        "                folium.Marker(\n",
        "                    location=[lat, lon],\n",
        "                    popup=f\"<b>Title:</b> {row['Title']}<br><b>Conflict Type:</b> {', '.join(row['conflict_info'])}<br><b>Species:</b> {', '.join(row['species'])}<br><a href='{row['Link']}'>Link</a>\",\n",
        "                    tooltip=row['Title'],\n",
        "                    icon=folium.Icon(color='red')\n",
        "                ).add_to(m)\n",
        "\n",
        "# Add markers for News data\n",
        "for idx, row in news_df.iterrows():\n",
        "    for loc in row[\"geocoded\"]:\n",
        "        if loc[\"latitude\"] and loc[\"longitude\"]:\n",
        "            lat, lon = loc[\"latitude\"], loc[\"longitude\"]\n",
        "            if is_within_india(lat, lon):\n",
        "                folium.Marker(\n",
        "                    location=[lat, lon],\n",
        "                    popup=f\"<b>Title:</b> {row['title']}<br><b>Conflict Type:</b> {', '.join(row['conflict_info'])}<br><b>Species:</b> {', '.join(row['species'])}<br><a href='{row['link']}'>Link</a>\",\n",
        "                    tooltip=row['title'],\n",
        "                    icon=folium.Icon(color='blue')\n",
        "                ).add_to(m)\n",
        "# displays the map\n",
        "m\n",
        "\n",
        "### Mapping and visualization block ends here\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}